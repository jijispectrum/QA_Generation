{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c45d5dc5-3fbd-476f-ae39-0ad2ba59c68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the job description:  Looking for Bank Manager Role\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Job Interview Question: What is the most important part of the job description?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your answer to the question:  Managing Bank Team\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Feedback:\n",
      " Good\n",
      "\n",
      "Suggested Answer:\n",
      " The Bank Manager must be able to manage a team of up to 20 people.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the FLAN-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate a job interview question based on the job description\n",
    "def generate_job_question(job_description):\n",
    "    prompt = f\"Generate a job interview question based on the job description: {job_description}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to generate an answer based on the job description and question\n",
    "def generate_job_answer(question, job_description):\n",
    "    prompt = f\"Job description: {job_description}  Question: {question}  Answer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to generate AI feedback with a rating\n",
    "def generate_feedback(question, user_answer, job_description):\n",
    "    prompt = f\"\"\"\n",
    "    Job description: {job_description}\n",
    "    Question: {question}\n",
    "    User's Answer: {user_answer}\n",
    "    \n",
    "    Evaluate the user's answer and provide a rating: \"Excellent,\" \"Good,\" or \"Needs Improvement.\"\n",
    "    Then, explain briefly why you gave that rating.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=150)  # Sufficient for rating and explanation\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to generate a suggested better answer\n",
    "def generate_suggested_answer(question, job_description):\n",
    "    prompt = f\"\"\"\n",
    "    Job description: {job_description}\n",
    "    Question: {question}\n",
    "    \n",
    "    Provide a well-structured and detailed suggested answer to this question.\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=200)  # Increased length for a complete answer\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Collect user input for the job description\n",
    "job_description_input = input(\"Enter the job description: \")\n",
    "\n",
    "# Generate a job interview question based on the job description\n",
    "generated_job_question = generate_job_question(job_description_input)\n",
    "print(\"\\nGenerated Job Interview Question:\", generated_job_question)\n",
    "\n",
    "# Collect user answer input\n",
    "user_answer_input = input(\"\\nEnter your answer to the question: \")\n",
    "\n",
    "# Generate AI feedback (rating + explanation)\n",
    "feedback = generate_feedback(generated_job_question, user_answer_input, job_description_input)\n",
    "print(\"\\nAI Feedback:\\n\", feedback)\n",
    "\n",
    "# Generate a suggested answer\n",
    "suggested_answer = generate_suggested_answer(generated_job_question, job_description_input)\n",
    "print(\"\\nSuggested Answer:\\n\", suggested_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb307e2-3b47-4afd-b855-a12aeaa734cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
